{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bf46350",
   "metadata": {},
   "source": [
    "## Accounting Measure Insensity: Replcation of Andreicovici, I. et al. (2022) \n",
    "\n",
    "复现：伊日敏（武汉大学）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2dc78b",
   "metadata": {},
   "source": [
    "### AMI构建原理\n",
    "\n",
    "#### **核心理念**\n",
    "\n",
    "[cite_start]首先，理解AMI指标的核心理念至关重要。该指标旨在量化企业在将经济交易映射到财务报表时所面临的“计量问题”（metering problems）。 [cite: 22, 42] \n",
    "\n",
    "[cite_start]作者认为，这些计量问题的严重程度，会反映在企业年度报告（10-K文件）中使用特定会计计量相关术语的“强度”上。 [cite: 22, 23]\n",
    "\n",
    "#### **复现步骤**\n",
    "\n",
    "构建AMI指标的过程可以分解为以下几个关键步骤：\n",
    "\n",
    "##### **第一步：构建训练语料库 (Training Libraries)**\n",
    "\n",
    "[cite_start]为了识别出会计领域特有的语言，研究人员构建了两个核心的语料库：**一个会计语料库 (A) 和一个非会计语料库 (N)**。 [cite: 182]\n",
    "\n",
    "1.  **构建会计语料库 (A):**\n",
    "    * [cite_start]**数据来源**: 该语料库包含了1973年至2019年间由**美国财务会计准则委员会（FASB）发布的各种权威文件**。 [cite: 223]\n",
    "    * **具体内容**:\n",
    "        * [cite_start]财务会计准则的原始公告和更新（FASB Original Pronouncements and Updates） [cite: 223]\n",
    "        * [cite_start]FASB的解释（FASB Interpretations） [cite: 223]\n",
    "        * [cite_start]FASB工作人员立场文件（FASB staff position papers） [cite: 223]\n",
    "        * [cite_start]新兴问题任务组摘要（Emerging Issues Task Force Abstracts） [cite: 223]\n",
    "    * [cite_start]**目标**: 这个语料库旨在捕捉会计专业人士在讨论和规定如何将经济交易转化为财务报告时所使用的独特词汇。 [cite: 222]\n",
    "\n",
    "2.  **构建非会计语料库 (N):**\n",
    "    * [cite_start]**目的**: 这个语料库的作用是筛选掉那些虽然在会计文件中出现，但也在日常或普通商业语境中广泛使用的词汇，从而确保最终选出的词组是会计领域“特有”的。 [cite: 181, 184, 186]\n",
    "    * **数据来源**:\n",
    "        * [cite_start]**基础部分**: 来自古腾堡计划（Project Gutenberg）的一系列英文小说。 [cite: 231, 996, 998]\n",
    "        * [cite_start]**补充部分**: 包含BBC新闻数据集和Webhose数据集中的新闻文章，主题涵盖娱乐、体育、科技和政治等，旨在捕捉更广泛的通用商业语言。 [cite: 189, 190, 231]\n",
    "    * [cite_start]**选择策略**: 作者特意选择面向普通受众的新闻文本，因为这类文本虽然可能包含商业词汇，但不大可能使用深奥的会计专用术语。 [cite: 191, 237]\n",
    "\n",
    "##### **第二步：文本预处理和词组识别**\n",
    "\n",
    "在正式进行词频分析之前，需要对所有文本数据进行标准化处理。\n",
    "\n",
    "1.  **文本预处理**:\n",
    "    * 对会计语料库（A）和非会计语料库（N）中的所有文本进行处理。\n",
    "    * **处理步骤**:\n",
    "        * [cite_start]**词形还原与词干提取 (Lemmatize and Stem)**: 将单词还原为其基本形式。 [cite: 200]\n",
    "        * [cite_start]**移除数字、标点和停用词 (Stop Words)**: 清理掉对语义分析无益的常见词（如 \"the\", \"is\", \"in\"）和符号。 [cite: 200]\n",
    "\n",
    "2.  **识别双词词组 (Bigrams)**:\n",
    "    * [cite_start]研究的核心分析单位是“双词词组”（bigrams），即两个连续的单词组合。 [cite: 182] [cite_start]作者认为双词词组比单个词更能捕捉到会计计量的具体概念（例如，“intangible asset”比单独的“asset”更具特定含义）。 [cite: 83, 85]\n",
    "\n",
    "##### **第三步：筛选独特的会计词组**\n",
    "\n",
    "这是构建指标的关键一步，目标是得到一个仅包含会计计量特有词组的词典。\n",
    "\n",
    "1.  **定义 accounting bigrams (AN)**:\n",
    "    * [cite_start]这个独特的会计词组集合的定义是：所有出现在会计语料库（A）中，但**不**出现在非会计语料库（N）中的双词词组。 [cite: 201]\n",
    "    * [cite_start]在论文中，这个集合表示为 **A \\ N**。 [cite: 201]\n",
    "\n",
    "2.  **过滤“样板”术语**:\n",
    "    * [cite_start]为了排除那些虽然是会计术语但过于普遍、无法区分公司间计量强度差异的“样板”词组，作者移除了在超过90%的10-K文件中都出现的双词词组。 [cite: 199]\n",
    "\n",
    "3.  **最终词典**:\n",
    "    * [cite_start]经过上述筛选，最终得到一个包含490,397个独特会计词组的词典。 [cite: 238] [cite_start]这些词组被认为是反映会计计量问题的“信号”。例如，“intangible asset”（无形资产）、“business combination”（企业合并）、“discount rate”（折现率）和“management estimate”（管理层估计）等。 [cite: 85, 243, 246]\n",
    "\n",
    "##### **第四步：计算公司层面的AMI分数**\n",
    "\n",
    "最后一步是将这个独特的会计词组词典应用到每个公司的年度报告中，以计算其AMI分数。\n",
    "\n",
    "1.  **数据来源**:\n",
    "    * [cite_start]2001年至2018年间，所有美国上市公司向美国证券交易委员会（SEC）提交的年度报告（10-K文件）。 [cite: 205]\n",
    "\n",
    "2.  **计算公式**:\n",
    "    * [cite_start]对于公司 *i* 在年份 *t* 的AMI分数，计算方法如下： [cite: 207]\n",
    "        $$AMI_{it}=\\frac{1}{B_{it}}\\sum_{b}^{B_{it}}(1[b\\in\\mathbb{A}\\backslash\\mathbb{N}]).$$\n",
    "    * **公式解释**:\n",
    "        * [cite_start]$b$ 代表公司 *i* 在年份 *t* 的10-K文件中出现的每一个双词词组。 [cite: 208]\n",
    "        * [cite_start]$B_{it}$ 是该10-K文件中双词词组的总数。 [cite: 208]\n",
    "        * [cite_start]$1[b\\in\\mathbb{A}\\backslash\\mathbb{N}]$ 是一个指示函数。如果词组 *b* 存在于之前构建的独特会计词组词典 (A \\ N) 中，则函数值为1，否则为0。 [cite: 208]\n",
    "        * [cite_start]**简而言之**: AMI分数就是一家公司年报中“独特会计词组”的数量，除以该年报中双词词组的总数。 [cite: 52, 203, 204] 这种标准化处理旨在衡量会计计量语言的“强度”或“密度”。\n",
    "\n",
    "3.  **权重方案**:\n",
    "    * [cite_start]在基础模型中，每个独特的会计词组被赋予相等的权重。 [cite: 209] [cite_start]作者这样做是为了避免少数高频词主导整个指标，从而更好地捕捉大量、异质化的计量专业术语。 [cite: 216]\n",
    "\n",
    "4.  **最终标准化**:\n",
    "    * [cite_start]为了便于解释和比较，最终的AMI分数被标准化处理：减去样本均值，再除以样本标准差。 [cite: 217] [cite_start]因此，一个单位的AMI变化代表一个标准差的变化。 [cite: 217]\n",
    "\n",
    "#### **复现总结**\n",
    "\n",
    "| 步骤 | 核心任务 | 数据和工具 | 关键产出 |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **1** | **构建语料库** | FASB官方文件、古腾堡计划小说、BBC新闻等。 | 会计语料库 (A) 和非会计语料库 (N)。 |\n",
    "| **2** | **文本预处理** | 词形还原、词干提取、移除停用词等文本分析技术。 | 清理和标准化后的文本数据。 |\n",
    "| **3** | **筛选词组** | 对比A和N语料库的词组，并过滤高频样板术语。 | [cite_start]包含490,397个独特会计双词词组的最终词典 (A \\ N)。 [cite: 238] |\n",
    "| **4** | **计算分数** | 上市公司10-K年报（2001-2018）。 | 每个公司每年的原始及标准化AMI分数。 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233c4406",
   "metadata": {},
   "source": [
    "### Python实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4510662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FileSystemPathPointer('/Users/mac/nltk_data')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.find('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ccd8de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac8cc566",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import punkt    # 用于分词\n",
    "from nltk.corpus import stopwords  # 用于获取停用词列表\n",
    "from nltk.corpus import wordnet# 用于词形还原"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd74d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import bigrams\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 初始化工具\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def process_text_to_bigrams(file_path):\n",
    "    \"\"\"\n",
    "    读取一个文本文件，进行预处理，并返回双词词组列表。\n",
    "    预处理步骤包括：小写化、去除非字母字符、分词、词形还原、去除停用词。\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "    except Exception as e:\n",
    "        print(f\"无法读取文件 {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 1. 转换为小写\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. 移除非字母字符 (保留单词和空格)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # 3. 分词\n",
    "    tokens = word_tokenize(text) # 输出结果是一个列表，包含文本中的所有单词\n",
    "    \n",
    "    # 4. 词形还原和去除停用词\n",
    "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words and len(word) > 1] # 这是一个列表推导式，遍历tokens中的每个单词，进行词形还原，并检查是否是停用词或长度小于2的单词\n",
    "    \n",
    "    # 5. 生成双词词组\n",
    "    # 将词组表示为 \"word1_word2\" 格式的字符串，便于处理\n",
    "    return [f\"{w1}_{w2}\" for w1, w2 in bigrams(processed_tokens)]\n",
    "\n",
    "# 这是一个辅助函数，用于处理整个目录的文件\n",
    "def process_directory(directory_path):\n",
    "    \"\"\"处理指定目录下的所有.txt文件，并返回一个包含所有双词词组的列表。\"\"\"\n",
    "    all_bigrams = []\n",
    "    file_list = [f for f in os.listdir(directory_path) if f.endswith('.txt')]\n",
    "    \n",
    "    # 使用tqdm来显示进度条\n",
    "    for filename in tqdm(file_list, desc=f\"处理目录 {directory_path}\"):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        bigrams_from_file = process_text_to_bigrams(file_path)\n",
    "        all_bigrams.extend(bigrams_from_file) # 将每个文件的双词词组添加到总列表中\n",
    "        \n",
    "    return all_bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddac597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 步骤 3: 构建独特的会计词组词典 ---\n",
      "\n",
      "正在处理会计语料库 (A)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理目录 fasb_docs/: 100%|██████████| 2/2 [00:00<00:00, 320.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从会计语料库中提取了 199 个双词词组。\n",
      "\n",
      "正在处理非会计语料库 (N)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "处理目录 non_acct_docs/: 100%|██████████| 2/2 [00:00<00:00, 400.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "从非会计语料库中提取了 140 个双词词组。\n",
      "\n",
      "正在计算独特的会计词组 (A \\ N)...\n",
      "成功创建独特会计词组词典！\n",
      "词典中的独特双词词组数量: 177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 步骤 3: 构建独特的会计词组词典 ---\")\n",
    "\n",
    "# 3.1 处理会计语料库 (A)\n",
    "print(\"\\n正在处理会计语料库 (A)...\")\n",
    "\n",
    "accounting_bigrams = process_directory('fasb_docs/') \n",
    "print(f\"从会计语料库中提取了 {len(accounting_bigrams)} 个双词词组。\")\n",
    "\n",
    "# 3.2 处理非会计语料库 (N)\n",
    "print(\"\\n正在处理非会计语料库 (N)...\")\n",
    "non_accounting_bigrams = process_directory('non_acct_docs/')\n",
    "print(f\"从非会计语料库中提取了 {len(non_accounting_bigrams)} 个双词词组。\")\n",
    "\n",
    "# 3.3 创建独特的会计词组词典 (A \\ N)\n",
    "print(\"\\n正在计算独特的会计词组 (A \\ N)...\")\n",
    "# 使用集合(set)可以极大地提高差集运算的效率\n",
    "accounting_set = set(accounting_bigrams)\n",
    "non_accounting_set = set(non_accounting_bigrams)\n",
    "\n",
    "unique_accounting_bigrams = accounting_set - non_accounting_set\n",
    "\n",
    "print(f\"成功创建独特会计词组词典！\")\n",
    "print(f\"词典中的独特双词词组数量: {len(unique_accounting_bigrams)}\")\n",
    "\n",
    "# ----------------- 修正后的代码块结束 -----------------\n",
    "# 论文中提到最终词典大小约为490,397 [cite: 238]，您的结果会因语料库的具体内容而异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25d3cb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 新增步骤: 开始过滤‘样板’会计词组 ---\n",
      "此步骤需要处理所有10-K文件，可能需要一些时间...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算文档频率(DF): 100%|██████████| 2/2 [00:00<00:00, 256.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "文档频率计算完成。\n",
      "总共处理了 2 份10-K报告。\n",
      "过滤阈值 (90%的文档数): 1.80\n",
      "识别出 1 个样板双词词组需要被移除。\n",
      "样板词组示例: ['fair_value']\n",
      "\n",
      "过滤前，独特会计词典大小: 177\n",
      "过滤后，独特会计词典大小: 176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------- 新增的步骤：过滤“样板”会计词组 -----------------\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"\\n--- 新增步骤: 开始过滤‘样板’会计词组 ---\")\n",
    "print(\"此步骤需要处理所有10-K文件，可能需要一些时间...\")\n",
    "\n",
    "# 1. 计算每个独特会计词组的文档频率 (DF)\n",
    "report_files = [f for f in os.listdir('10k_reports/') if f.endswith('.txt')]\n",
    "total_num_reports = len(report_files)\n",
    "\n",
    "# 使用defaultdict可以简化计数过程\n",
    "bigram_doc_frequency = defaultdict(int)\n",
    "\n",
    "for filename in tqdm(report_files, desc=\"计算文档频率(DF)\"):\n",
    "    file_path = os.path.join('10k_reports/', filename)\n",
    "    \n",
    "    # 获取当前报告的所有双词词组\n",
    "    report_bigrams_list = process_text_to_bigrams(file_path)\n",
    "    \n",
    "    # 关键：将列表转为集合，确保每个词组在单篇文档中只被计数一次\n",
    "    unique_bigrams_in_report = set(report_bigrams_list)\n",
    "    \n",
    "    # 遍历报告中的独特词组，更新它们的文档频率\n",
    "    for bigram in unique_bigrams_in_report:\n",
    "        # 我们只关心那些已经存在于我们会计词典中的词组\n",
    "        if bigram in unique_accounting_bigrams:\n",
    "            bigram_doc_frequency[bigram] += 1\n",
    "\n",
    "print(\"\\n文档频率计算完成。\")\n",
    "\n",
    "# 2. 识别需要被移除的样板词组\n",
    "# 论文中定义的阈值是90%的公司 \n",
    "threshold = total_num_reports * 0.9\n",
    "boilerplate_bigrams = set()\n",
    "\n",
    "for bigram, frequency in bigram_doc_frequency.items():\n",
    "    if frequency > threshold:\n",
    "        boilerplate_bigrams.add(bigram)\n",
    "\n",
    "print(f\"总共处理了 {total_num_reports} 份10-K报告。\")\n",
    "print(f\"过滤阈值 (90%的文档数): {threshold:.2f}\")\n",
    "print(f\"识别出 {len(boilerplate_bigrams)} 个样板双词词组需要被移除。\")\n",
    "if len(boilerplate_bigrams) > 0:\n",
    "    print(\"样板词组示例:\", list(boilerplate_bigrams)[:5]) # 打印前5个看看\n",
    "\n",
    "# 3. 从独特会计词典中移除样板词组\n",
    "print(f\"\\n过滤前，独特会计词典大小: {len(unique_accounting_bigrams)}\")\n",
    "\n",
    "# 使用集合的差集运算进行过滤\n",
    "filtered_unique_accounting_bigrams = unique_accounting_bigrams - boilerplate_bigrams\n",
    "\n",
    "print(f\"过滤后，独特会计词典大小: {len(filtered_unique_accounting_bigrams)}\")\n",
    "\n",
    "# ----------------- 新增步骤结束 -----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "39e87b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 4: 开始计算每份10-K报告的AMI分数 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算10-K报告的AMI: 100%|██████████| 2/2 [00:00<00:00, 662.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AMI分数计算完成！\n",
      "                          filename  ...  unique_bigrams_count\n",
      "0  global_manufacturing_co_10k.txt  ...                     1\n",
      "1     tech_innovations_inc_10k.txt  ...                     5\n",
      "\n",
      "[2 rows x 4 columns]\n",
      "\n",
      "原始AMI分数已保存到 ami_scores_raw.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 步骤 4: 开始计算每份10-K报告的AMI分数 ---\")\n",
    "report_files = [f for f in os.listdir('10k_reports/') if f.endswith('.txt')]\n",
    "ami_results = []\n",
    "\n",
    "for filename in tqdm(report_files, desc=\"计算10-K报告的AMI\"):\n",
    "    file_path = os.path.join('10k_reports/', filename)\n",
    "    \n",
    "    # 1. 提取当前报告的所有双词词组\n",
    "    report_bigrams = process_text_to_bigrams(file_path)\n",
    "    \n",
    "    # 2. 获取双词词组总数 (公式中的 B_it)\n",
    "    total_bigrams_in_report = len(report_bigrams)\n",
    "    \n",
    "    if total_bigrams_in_report == 0:\n",
    "        continue\n",
    "    \n",
    "    # 3. 计算报告中出现在我们独特词典里的词组数量\n",
    "    unique_count = 0\n",
    "    for bigram in report_bigrams:\n",
    "        if bigram in filtered_unique_accounting_bigrams:\n",
    "            unique_count += 1\n",
    "            \n",
    "    # 4. 计算原始AMI分数\n",
    "    raw_ami = unique_count / total_bigrams_in_report\n",
    "    \n",
    "    ami_results.append({\n",
    "        'filename': filename,\n",
    "        'raw_ami': raw_ami,\n",
    "        'total_bigrams': total_bigrams_in_report,\n",
    "        'unique_bigrams_count': unique_count\n",
    "    })\n",
    "\n",
    "# 将结果转换为Pandas DataFrame以便于分析\n",
    "df_ami = pd.DataFrame(ami_results)\n",
    "\n",
    "print(\"\\nAMI分数计算完成！\")\n",
    "print(df_ami.head())\n",
    "\n",
    "# 保存结果到CSV文件\n",
    "df_ami.to_csv('ami_scores_raw.csv', index=False)\n",
    "print(\"\\n原始AMI分数已保存到 ami_scores_raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4368740e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 步骤 5: 标准化AMI分数 ---\n",
      "原始AMI分数的均值: 0.023698\n",
      "原始AMI分数的标准差: 0.021729\n",
      "\n",
      "标准化后的AMI分数预览:\n",
      "                          filename  ...  ami_standardized\n",
      "0  global_manufacturing_co_10k.txt  ...         -0.707107\n",
      "1     tech_innovations_inc_10k.txt  ...          0.707107\n",
      "\n",
      "[2 rows x 5 columns]\n",
      "\n",
      "最终AMI分数（包含标准化值）已保存到 ami_scores_final.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- 步骤 5: 标准化AMI分数 ---\")\n",
    "# 计算均值和标准差\n",
    "ami_mean = df_ami['raw_ami'].mean()\n",
    "ami_std = df_ami['raw_ami'].std()\n",
    "\n",
    "print(f\"原始AMI分数的均值: {ami_mean:.6f}\")\n",
    "print(f\"原始AMI分数的标准差: {ami_std:.6f}\")\n",
    "\n",
    "# 计算标准化AMI分数\n",
    "df_ami['ami_standardized'] = (df_ami['raw_ami'] - ami_mean) / ami_std\n",
    "\n",
    "print(\"\\n标准化后的AMI分数预览:\")\n",
    "print(df_ami.head())\n",
    "\n",
    "# 保存包含标准化分数的结果\n",
    "df_ami.to_csv('ami_scores_final.csv', index=False)\n",
    "print(\"\\n最终AMI分数（包含标准化值）已保存到 ami_scores_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e268422",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bea5efb3",
   "metadata": {},
   "source": [
    "### 对AMI指标的检验\n",
    "\n",
    "根据您提供的论文，在构建了会计计量强度（AMI）指标之后，研究人员进行了一系列的统计学检验，其核心目的可以归纳为两个方面：**一是验证AMI指标本身的有效性（face validity and construct validity）**，即证明它确实衡量了预期的“计量问题”；**二是利用该指标检验源于公司理论和交易成本经济学的核心假设**，即计量问题如何影响公司的边界、生产率和合同。\n",
    "\n",
    "以下是论文中执行的主要统计学检验及其验证目的：\n",
    "\n",
    "#### **第一部分：验证AMI指标的有效性**\n",
    "\n",
    "这部分的检验旨在证明AMI是一个有意义且可靠的度量。\n",
    "\n",
    "1.  **描述性统计和可视化检验 (Face Validity)**\n",
    "    * **检验内容**:\n",
    "        * [cite_start]列出词典中频率最高的双词词组（bigrams），以直观展示这些词组确实与会计计量相关，如“intangible asset”（无形资产）、“business combination”（企业合并）、“discount rate”（折现率）等 [cite: 83-85]。\n",
    "        * [cite_start]展示AMI分数最高的一些标准普尔500指数公司及其年报中的相关文本片段，以证明高分数的公司确实在广泛讨论计量问题 [cite: 86, 277-278]。\n",
    "        * [cite_start]绘制AMI随时间变化的趋势图（图1），显示其均值在2008年之前呈上升趋势，之后趋于平稳，这与公允价值会计准则的发展历程相吻合 [cite: 97, 260-262]。\n",
    "        * [cite_start]绘制AMI在不同行业的均值图（图2），显示其在金融服务、建筑业等行业得分较高，而在烟草、煤炭等行业得分较低，符合直觉预期 [cite: 99, 262-264]。\n",
    "    * **验证目的**: 建立指标的“表面有效性”，即证明该指标从直觉和感官上是合理且可信的。\n",
    "\n",
    "2.  **方差分解分析 (Variance Decomposition)**\n",
    "    * [cite_start]**检验内容**: 通过回归分析，将AMI总方差分解为时间效应、行业效应和公司层面的效应 [cite: 265][cite_start]。分析发现，大部分（约69%）的变动发生在公司层面，证明AMI主要捕捉的是公司特有的异质性，而非宏观或行业层面的普遍趋势 [cite: 269]。\n",
    "    * **验证目的**: 检验AMI变异的主要来源，证明其作为一个公司级别的指标是有效的。\n",
    "\n",
    "3.  **与公司特征的关联性分析**\n",
    "    * [cite_start]**检验内容**: 检验AMI与一系列被认为和会计计量问题相关的公司特征之间的关系 [cite: 100][cite_start]。结果显示，AMI与公司经营环境的波动性、公司规模、经营周期的长度、无形资产密集度和资本密集度呈正相关 [cite: 315-316]。\n",
    "    * **验证目的**: 建立指标的“构建有效性”（construct validity），即证明AMI与其他理论上相关的构念存在预期的关联。\n",
    "\n",
    "4.  **与审计费用的回归分析**\n",
    "    * [cite_start]**检验内容**: 将审计费用对AMI进行回归，同时控制公司规模、业绩、资本结构以及其他会计质量指标（如盈余持续性、可预测性）等变量 [cite: 295-296][cite_start]。结果显示，AMI与审计费用显著正相关，即AMI越高的公司，审计师收取的费用也越高 [cite: 307-309]。\n",
    "    * [cite_start]**验证目的**: 验证外部专家（审计师）是否将AMI所捕捉的计量问题识别为一种需要投入更多审计努力和承担更高风险的因素，并将其反映在服务定价中 [cite: 100-101, 285]。\n",
    "\n",
    "#### **第二部分：利用AMI检验经济学理论**\n",
    "\n",
    "这部分的检验将AMI作为核心自变量，去探索“计量问题”的经济后果。\n",
    "\n",
    "1.  **对公司成长和投资的检验**\n",
    "    * [cite_start]**检验内容**: 分别将公司的资本支出、无形资产投资（研发）和雇佣增长率对AMI进行回归 [cite: 345-347][cite_start]。结果显示，AMI与这三项指标均呈显著负相关 [cite: 115, 359-361]。\n",
    "    * [cite_start]**验证目的**: 检验公司理论中的核心预测，即计量问题会限制公司的成长和扩张，从而定义公司的边界 [cite: 103, 113-114]。\n",
    "\n",
    "2.  **对公司生产率的检验**\n",
    "    * [cite_start]**检验内容**: 将全要素生产率（TFP）和托宾Q值（Tobin's Q）对AMI进行回归 [cite: 381, 399][cite_start]。结果发现，AMI与TFP和托宾Q值均呈显著负相关 [cite: 120-121, 385, 399-400]。\n",
    "    * [cite_start]**验证目的**: 检验计量问题是否通过扭曲资源配置或激励机制，最终损害了公司的生产效率和盈利性投资机会 [cite: 118-119]。\n",
    "\n",
    "3.  **对资本市场摩擦的检验**\n",
    "    * **检验内容**:\n",
    "        * [cite_start]检验AMI与股权市场信息不对称代理指标（如知情交易概率PIN和分析师覆盖度）的关系 [cite: 124-125, 424]。\n",
    "        * [cite_start]检验AMI与债务市场融资成本（银行贷款利差）的关系 [cite: 127, 436]。\n",
    "        * [cite_start]检验AMI对公司投资-现金流敏感性的影响 [cite: 131, 457-458]。\n",
    "    * [cite_start]**验证目的**: 检验计量问题是否会增加外部投资者获取信息的难度，从而导致更高的资本成本和更强的融资约束 [cite: 122-123]。\n",
    "\n",
    "4.  **对合同设计的检验**\n",
    "    * **检验内容**:\n",
    "        * [cite_start]检验AMI与债务合同中财务契约数量的关系 [cite: 134-135, 491-492]。\n",
    "        * [cite_start]检验AMI如何调节CEO薪酬对会计业绩（ROA）和股票表现的敏感性 [cite: 136, 497-498, 501-503]。\n",
    "    * [cite_start]**验证目的**: 检验计量问题是否会影响公司与内外部利益相关者（如债权人、高管）签订的合同条款，以应对更高的监督和激励成本 [cite: 133, 478]。\n",
    "\n",
    "5.  **稳健性检验 (Falsification and Alternative Explanations)**\n",
    "    * [cite_start]**检验内容**: 在核心回归模型中加入了其他可能混淆结果的变量，如用文件大小衡量的“披露复杂性”、传统的会计质量代理变量（如酌情性应计），并构建了一个不含公允价值相关词组的AMI版本进行测试 [cite: 528-529, 531-532, 545-546, 550-553][cite_start]。此外，还进行了安慰剂检验（placebo tests） [cite: 561]。\n",
    "    * **验证目的**: 排除其他可能的解释，证明AMI捕捉的是一种独特的、不同于传统披露复杂性或会计质量的经济构念，并且检验结果不是由随机因素驱动的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7289e",
   "metadata": {},
   "source": [
    "#### 方差分解为什么重要？\n",
    "\n",
    "方差分解分析（Variance Decomposition Analysis）之所以重要，是因为它能帮助研究者理解一个变量（如此处的AMI指标）的总变动中，有多少可以归因于不同的影响来源。简单来说，它回答了这样一个问题：**“我们所观察到的差异，主要是由哪个层面的因素驱动的？”**\n",
    "\n",
    "这在验证一个新构建的指标时尤其关键。以下是其重要性的几个方面，并结合论文中的应用进行说明：\n",
    "\n",
    "##### 1. **验证指标的核心属性 (Validating the Core Attribute of the Indicator)**\n",
    "\n",
    "在构建AMI指标时，研究者的初衷是创造一个能够衡量**公司层面（firm-level）**异质性计量问题的指标。如果这个指标的绝大部分变动仅仅是由宏观经济随时间的变化（时间效应）或行业普遍特征（行业效应）所驱动，那么这个指标作为衡量“公司个体”特征的工具就是失败的。\n",
    "\n",
    "* **论文中的应用**:\n",
    "    * [cite_start]研究者通过方差分解发现，时间固定效应（即宏观经济随时间的变化）仅能解释AMI总变动的 **5.53%** [cite: 267]。这证明了AMI并不仅仅是一个反映会计准则随时间演变的简单时间趋势变量。\n",
    "    * [cite_start]行业固定效应（即公司所处行业的共性）能解释约 **21.29%** 的变动 [cite: 268]。这符合预期，因为建筑、金融等行业的计量问题本身就比零售业更复杂。\n",
    "    * [cite_start]最关键的发现是，剩余的、高达 **69.98%** 的变动发生在公司层面 [cite: 269]。\n",
    "* **重要性**: 这一结果强有力地证明了AMI指标确实成功捕捉到了公司与公司之间的个体差异，而不是宏观或行业的普遍趋势。这为论文后续所有基于“公司异质性”的分析提供了坚实的基础和合法性。\n",
    "\n",
    "##### 2. **识别主要驱动因素 (Identifying Key Drivers)**\n",
    "\n",
    "方差分解能够清晰地揭示出影响一个现象的最主要因素，帮助研究者抓住问题的关键。它将一个复杂的、混合的总体效应，拆解成几个独立的、可解释的部分，让我们知道应该将注意力集中在哪里。\n",
    "\n",
    "* **论文中的应用**: 结果显示，虽然行业背景有一定影响，但决定一家公司计量强度高低的最主要因素是公司自身的特点和行为。这就引导研究者和读者去思考：是什么样的公司战略、业务模式或治理结构导致了更高的AMI？\n",
    "\n",
    "##### 3. **指导后续的统计模型设定 (Guiding Future Model Specification)**\n",
    "\n",
    "方差分解的结果对后续研究中如何设定回归模型具有重要的指导意义。\n",
    "\n",
    "* **论文中的应用**:\n",
    "    * 既然已经证实了公司层面的效应是AMI变动的主要来源，那么在后续检验AMI的经济后果时，就必须有效地控制这些不随时间变化的公司个体特征。\n",
    "    * 因此，在论文的许多核心回归表格中（如Table 3到Table 11），作者们都采用了包含**公司固定效应（Firm FE）**的模型。这种模型设定可以排除掉所有不随时间变化的公司固有属性（如企业文化、长期战略等）的干扰，从而更准确地识别出AMI在**公司内部随时间变化**时所带来的影响。\n",
    "    * 可以说，方差分解的结果为后续采用公司固定效应模型提供了理论依据。\n",
    "\n",
    "综上所述，方差分解分析在本文中扮演了一个至关重要的**承上启下**的角色。它**向上验证**了AMI指标构建的成功性，证明了它是一个有效的公司层面度量；它**向下指导**了后续实证检验的模型设定，确保了研究结论的严谨性和可靠性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
