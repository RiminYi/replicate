{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1747c4f4",
   "metadata": {},
   "source": [
    "# 项目复现主流程 (Project Replication Main Pipeline)\n",
    "\n",
    "本 Notebook 将作为项目的主控制台，引导您按顺序执行 `src/` 文件夹中的各个核心脚本，完成从数据准备到关键词发现的整个端到端流程。\n",
    "\n",
    "**运行前请注意:**\n",
    "* 请确保您已经安装了 `requirements.txt` 中的所有库 (`pip install -r requirements.txt`)。\n",
    "* 请确保您是从项目的根目录 (`climate_keyword_discovery/`) 启动 Jupyter Notebook/Lab。\n",
    "* 每个代码单元格对应算法的一个主要步骤。请按顺序执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfddb83d",
   "metadata": {},
   "source": [
    "### 准备工作：检查路径和数据\n",
    "\n",
    "在开始前，我们先定义好所有需要的路径，并快速检查一下原始数据是否就位。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c0d0d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 准备工作检查通过: 种子词组文件位于 'data/raw/initial_seed_bigrams.csv'\n",
      "\n",
      "项目路径设置完毕，可以开始执行流程。\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# --- 定义项目核心路径 ---\n",
    "RAW_DATA_PATH = os.path.join('data', 'raw')\n",
    "PROCESSED_DATA_PATH = os.path.join('data', 'processed')\n",
    "MODELS_PATH = 'models'\n",
    "RESULTS_PATH = 'results'\n",
    "SRC_PATH = 'src'\n",
    "\n",
    "# --- 检查关键文件是否存在 ---\n",
    "seed_file = os.path.join(RAW_DATA_PATH, 'initial_seed_bigrams.csv')\n",
    "\n",
    "if not os.path.exists(seed_file):\n",
    "    print(f\"❌ 错误: 找不到种子词组文件，请确保 '{seed_file}' 文件存在！\")\n",
    "else:\n",
    "    print(f\"✅ 准备工作检查通过: 种子词组文件位于 '{seed_file}'\")\n",
    "\n",
    "print(\"\\n项目路径设置完毕，可以开始执行流程。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02794962",
   "metadata": {},
   "source": [
    "---\n",
    "## 步骤 1: 构建参照集 (R) 和搜索集 (S)\n",
    "\n",
    "我们将执行 `src/set_builder.py` 脚本。\n",
    "\n",
    "该脚本会：\n",
    "1.  加载 `data/raw/initial_seed_bigrams.csv` 中的种子词组。\n",
    "2.  遍历 `data/raw/` 目录下的所有 `.txt` 文件并进行预处理。\n",
    "3.  根据句子是否包含种子词组，将其分别归入参照集 (R) 和搜索集 (S)。\n",
    "4.  将结果保存到 `data/processed/` 目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd51ebe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始执行步骤 1: 构建 R 集和 S 集 ---\n",
      "Loaded 50 seed bigrams.\n",
      "Preprocessing file: data/raw/non_climate_texts/fiction_excerpt.txt\n",
      "Preprocessing file: data/raw/non_climate_texts/local_news_article.txt\n",
      "Preprocessing file: data/raw/transcripts/autotech_q3_2025.txt\n",
      "Preprocessing file: data/raw/transcripts/energy_co_q3_2025.txt\n",
      "Preprocessing file: data/raw/ipcc_reports/chapter_3_physical_basis.txt\n",
      "Preprocessing file: data/raw/ipcc_reports/chapter_5_mitigation_pathways.txt\n",
      "\n",
      "--- Set Building Complete ---\n",
      "Reference Set (R) contains: 17 sentences.\n",
      "Search Set (S) contains: 50 sentences.\n",
      "Reference Set (R) saved to: data/processed/reference_set_R.json\n",
      "Search Set (S) saved to: data/processed/search_set_S.json\n",
      "\n",
      "--- 步骤 1 执行完毕 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 开始执行步骤 1: 构建 R 集和 S 集 ---\")\n",
    "%run src/set_builder.py\n",
    "print(\"\\n--- 步骤 1 执行完毕 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7299533e",
   "metadata": {},
   "source": [
    "---\n",
    "## 步骤 2: 训练机器学习模型\n",
    "\n",
    "现在，我们将执行 `src/model_trainer.py`。\n",
    "\n",
    "该脚本会：\n",
    "1.  加载上一步生成的 R 集和 S 集。\n",
    "2.  构建训练集（R 作为正样本，S 的抽样作为负样本）。\n",
    "3.  使用 TF-IDF 将文本向量化。\n",
    "4.  通过网格搜索交叉验证来训练和调优三个分类器（朴素贝叶斯、SVC、随机森林）。\n",
    "5.  将训练好的最佳模型保存到 `models/` 目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3345d47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始执行步骤 2: 训练模型 (这可能需要一些时间) ---\n",
      "--- Loading Data ---\n",
      "Loaded 17 sentences for Reference Set (R).\n",
      "Loaded 50 sentences for Search Set (S).\n",
      "\n",
      "--- Preparing Training Data ---\n",
      "Created training set with 67 samples.\n",
      "Vectorizing text data with TF-IDF...\n",
      "Vectorizer saved.\n",
      "\n",
      "--- Training NaiveBayes ---\n",
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best parameters for NaiveBayes: {'alpha': 0.5}\n",
      "NaiveBayes model saved to: models/naivebayes_classifier.joblib\n",
      "\n",
      "--- Training SVC ---\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Best parameters for SVC: {'C': 10, 'gamma': 'scale', 'kernel': 'linear'}\n",
      "SVC model saved to: models/svc_classifier.joblib\n",
      "\n",
      "--- Training RandomForest ---\n",
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n",
      "Best parameters for RandomForest: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "RandomForest model saved to: models/randomforest_classifier.joblib\n",
      "\n",
      "--- 步骤 2 执行完毕 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 开始执行步骤 2: 训练模型 (这可能需要一些时间) ---\")\n",
    "%run src/model_trainer.py\n",
    "print(\"\\n--- 步骤 2 执行完毕 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77510a96",
   "metadata": {},
   "source": [
    "**结果验证:**\n",
    "\n",
    "这个过程会花费一些时间。运行结束后，您应该会在 `models` 目录下看到四个 `.joblib` 文件：\n",
    "* `tfidf_vectorizer.joblib`\n",
    "* `naivebayes_classifier.joblib`\n",
    "* `svc_classifier.joblib`\n",
    "* `randomforest_classifier.joblib`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7164e0e",
   "metadata": {},
   "source": [
    "---\n",
    "## 步骤 3: 预测并生成目标集 (T)\n",
    "\n",
    "接下来，执行 `src/predictor.py`。\n",
    "\n",
    "该脚本会：\n",
    "1.  加载上一步训练好的模型和向量化器。\n",
    "2.  使用模型对搜索集 (S) 中的每一个句子进行概率预测。\n",
    "3.  将任何一个模型预测概率超过阈值（我们在脚本中设为0.5）的句子筛选出来，构成目标集 (T)。\n",
    "4.  将结果保存到 `data/processed/` 目录下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7db6632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始执行步骤 3: 预测并生成 T 集 ---\n",
      "--- Loading Models and Vectorizer ---\n",
      "Successfully loaded vectorizer and all 3 models.\n",
      "\n",
      "--- Loading Search Set (S) ---\n",
      "Scanning 50 sentences in the Search Set (S)...\n",
      "\n",
      "--- Prediction Complete ---\n",
      "Found 9 sentences for the Target Set (T).\n",
      "Target Set (T) saved to: data/processed/target_set_T.json\n",
      "\n",
      "--- 步骤 3 执行完毕 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 开始执行步骤 3: 预测并生成 T 集 ---\")\n",
    "%run src/predictor.py\n",
    "print(\"\\n--- 步骤 3 执行完毕 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdeddb90",
   "metadata": {},
   "source": [
    "**结果验证:**\n",
    "\n",
    "运行结束后，在 `data/processed` 目录下，您会看到新生成的 `target_set_T.json` 文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439d1ac3",
   "metadata": {},
   "source": [
    "---\n",
    "## 步骤 4: 发现新关键词并生成最终词库\n",
    "\n",
    "这是流程的最后一步，我们将执行 `src/keyword_discoverer.py`。\n",
    "\n",
    "该脚本会：\n",
    "1.  加载 T 集、S 集和初始种子词组。\n",
    "2.  通过比较词组在 T 集和非 T 集中的文档频率，来发现新的关键词。\n",
    "3.  将新发现的关键词与初始种子词组合并，形成最终词库。\n",
    "4.  将最终词库保存为 `final_bigram_library.csv`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0c1e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 开始执行步骤 4: 发现新关键词 ---\n",
      "--- Loading T, S, and initial seed bigrams ---\n",
      "Target Set (T) size: 9\n",
      "Non-Target Set (S\\T) size: 41\n",
      "\n",
      "--- Calculating Document Frequencies ---\n",
      "--- Calculating likelihood scores for candidate bigrams ---\n",
      "Kept top 5% of candidates: 3 new bigrams.\n",
      "\n",
      "--- Final Bigram Library Created ---\n",
      "Total bigrams in the final library: 53\n",
      "Final bigram library saved to: data/processed/final_bigram_library.csv\n",
      "\n",
      "--- Example new bigrams (if any) ---\n",
      "[('Kyoto', 'protocol'), ('air', 'pollution'), ('air', 'quality'), ('air', 'temperature'), ('biomass', 'energy'), ('carbon', 'dioxide'), ('carbon', 'emission'), ('carbon', 'energy'), ('carbon', 'neutral'), ('carbon', 'price')]\n",
      "\n",
      "--- 步骤 4 执行完毕 ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 开始执行步骤 4: 发现新关键词 ---\")\n",
    "%run src/keyword_discovery_v2.py\n",
    "print(\"\\n--- 步骤 4 执行完毕 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c50f6",
   "metadata": {},
   "source": [
    "**结果验证:**\n",
    "\n",
    "最终的成果 `final_bigram_library.csv` 会保存在 `data/processed` 目录下。您可以打开这个文件查看所有被发现的关键词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121650c1",
   "metadata": {},
   "source": [
    "---\n",
    "## 结论\n",
    "\n",
    "恭喜您！您已经成功地通过这个 Notebook 完整地运行了整个关键词发现算法的核心流程。\n",
    "\n",
    "您现在拥有一个完整的、可以工作的关键词发现管道。接下来，您可以：\n",
    "* 在 `notebooks` 文件夹下创建新的 Notebook，对 `final_bigram_library.csv` 进行分析。\n",
    "* 编写 `src/exposure_calculator.py` 脚本来应用这个新词库，计算最终的风险敞口分数。\n",
    "* 用更大规模、更真实的数据来替换 `data/raw` 中的文件，然后重新运行此 Notebook，看看能有什么新发现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51657f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
